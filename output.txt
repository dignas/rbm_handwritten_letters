[BernoulliRBM] Iteration 1, pseudo-likelihood = -154.32, time = 163.25s
[BernoulliRBM] Iteration 2, pseudo-likelihood = -132.63, time = 185.99s
[BernoulliRBM] Iteration 3, pseudo-likelihood = -122.37, time = 183.11s
[BernoulliRBM] Iteration 4, pseudo-likelihood = -116.92, time = 183.66s
[BernoulliRBM] Iteration 5, pseudo-likelihood = -112.59, time = 181.82s
[BernoulliRBM] Iteration 6, pseudo-likelihood = -109.46, time = 183.79s
[BernoulliRBM] Iteration 7, pseudo-likelihood = -108.16, time = 184.31s
[BernoulliRBM] Iteration 8, pseudo-likelihood = -106.01, time = 182.03s
[BernoulliRBM] Iteration 9, pseudo-likelihood = -104.94, time = 182.34s
[BernoulliRBM] Iteration 10, pseudo-likelihood = -103.16, time = 183.37s
[BernoulliRBM] Iteration 11, pseudo-likelihood = -102.77, time = 180.83s
[BernoulliRBM] Iteration 12, pseudo-likelihood = -102.96, time = 182.26s
[BernoulliRBM] Iteration 13, pseudo-likelihood = -100.69, time = 180.94s
[BernoulliRBM] Iteration 14, pseudo-likelihood = -100.47, time = 182.52s
[BernoulliRBM] Iteration 15, pseudo-likelihood = -98.59, time = 180.74s
[BernoulliRBM] Iteration 16, pseudo-likelihood = -99.11, time = 183.10s
[BernoulliRBM] Iteration 17, pseudo-likelihood = -97.73, time = 190.51s
[BernoulliRBM] Iteration 18, pseudo-likelihood = -98.12, time = 181.01s
[BernoulliRBM] Iteration 19, pseudo-likelihood = -96.45, time = 180.58s
[BernoulliRBM] Iteration 20, pseudo-likelihood = -97.62, time = 179.34s
[BernoulliRBM] Iteration 21, pseudo-likelihood = -97.22, time = 181.90s
[BernoulliRBM] Iteration 22, pseudo-likelihood = -95.85, time = 182.13s
[BernoulliRBM] Iteration 23, pseudo-likelihood = -96.53, time = 182.58s
[BernoulliRBM] Iteration 24, pseudo-likelihood = -95.57, time = 181.27s
[BernoulliRBM] Iteration 25, pseudo-likelihood = -95.33, time = 182.23s
[BernoulliRBM] Iteration 26, pseudo-likelihood = -95.18, time = 183.14s
[BernoulliRBM] Iteration 27, pseudo-likelihood = -94.34, time = 183.71s
[BernoulliRBM] Iteration 28, pseudo-likelihood = -94.73, time = 182.89s
[BernoulliRBM] Iteration 29, pseudo-likelihood = -94.64, time = 182.29s
[BernoulliRBM] Iteration 30, pseudo-likelihood = -95.50, time = 181.98s
[BernoulliRBM] Iteration 31, pseudo-likelihood = -94.88, time = 180.28s
[BernoulliRBM] Iteration 32, pseudo-likelihood = -95.72, time = 180.57s
[BernoulliRBM] Iteration 33, pseudo-likelihood = -93.82, time = 187.39s
[BernoulliRBM] Iteration 34, pseudo-likelihood = -93.83, time = 181.13s
[BernoulliRBM] Iteration 35, pseudo-likelihood = -93.82, time = 181.16s
[BernoulliRBM] Iteration 36, pseudo-likelihood = -93.73, time = 181.07s
[BernoulliRBM] Iteration 37, pseudo-likelihood = -93.72, time = 181.20s
[BernoulliRBM] Iteration 38, pseudo-likelihood = -93.72, time = 180.87s
[BernoulliRBM] Iteration 39, pseudo-likelihood = -92.81, time = 180.65s
[BernoulliRBM] Iteration 40, pseudo-likelihood = -94.11, time = 180.89s
[BernoulliRBM] Iteration 41, pseudo-likelihood = -92.17, time = 180.41s
[BernoulliRBM] Iteration 42, pseudo-likelihood = -93.29, time = 180.57s
[BernoulliRBM] Iteration 43, pseudo-likelihood = -92.55, time = 180.87s
[BernoulliRBM] Iteration 44, pseudo-likelihood = -93.21, time = 182.04s
[BernoulliRBM] Iteration 45, pseudo-likelihood = -92.23, time = 180.90s
[BernoulliRBM] Iteration 46, pseudo-likelihood = -92.13, time = 181.35s
[BernoulliRBM] Iteration 47, pseudo-likelihood = -93.33, time = 181.21s
[BernoulliRBM] Iteration 48, pseudo-likelihood = -92.91, time = 181.27s
[BernoulliRBM] Iteration 49, pseudo-likelihood = -91.95, time = 181.31s
[BernoulliRBM] Iteration 50, pseudo-likelihood = -92.19, time = 189.59s
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =        67650     M =           10
 This problem is unconstrained.

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.18965D+00    |proj g|=  1.61951D-02

At iterate   50    f=  8.80700D-01    |proj g|=  1.27755D-02

At iterate  100    f=  5.21254D-01    |proj g|=  2.07731D-03

At iterate  150    f=  4.11849D-01    |proj g|=  7.43492D-04

At iterate  200    f=  3.68651D-01    |proj g|=  8.35687D-04

At iterate  250    f=  3.50314D-01    |proj g|=  3.42124D-04

At iterate  300    f=  3.41779D-01    |proj g|=  8.04966D-04

At iterate  350    f=  3.37367D-01    |proj g|=  1.13250D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
67650    362    370      1     0     0   9.938D-05   3.367D-01
  F =  0.33667919511017358     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =        67650     M =           10
 This problem is unconstrained.

At X0         0 variables are exactly at the bounds

At iterate    0    f=  4.18965D+00    |proj g|=  1.26208D-02

At iterate   50    f=  1.99350D+00    |proj g|=  1.05745D-03

At iterate  100    f=  1.95010D+00    |proj g|=  6.05128D-04

At iterate  150    f=  1.93387D+00    |proj g|=  8.26326D-04

At iterate  200    f=  1.92578D+00    |proj g|=  3.91968D-04

At iterate  250    f=  1.92096D+00    |proj g|=  2.23537D-04

At iterate  300    f=  1.91797D+00    |proj g|=  4.23626D-04

At iterate  350    f=  1.91608D+00    |proj g|=  1.41267D-04

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
67650    361    372      1     0     0   9.793D-05   1.916D+00
  F =   1.9157800746554230     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RBM + logistic classification result:
               precision    recall  f1-score   support

           0       0.66      0.69      0.67       115
           1       0.92      0.87      0.89       141
           2       0.80      0.83      0.82       170
           3       0.87      0.87      0.87       227
           4       0.80      0.75      0.77       173
           5       0.74      0.85      0.79       215
           6       0.84      0.89      0.86       183
           7       0.76      0.70      0.73       193
           8       0.86      0.84      0.85       216
           9       0.71      0.66      0.68       152
          10       0.73      0.67      0.70        92
          11       0.75      0.68      0.71       186
          12       0.83      0.84      0.83       152
          13       0.78      0.81      0.80       151
          14       0.81      0.74      0.77       152
          15       0.88      0.86      0.87       138
          16       0.78      0.79      0.78       155
          17       0.92      0.93      0.93       130
          18       0.81      0.78      0.80       188
          19       0.77      0.77      0.77       196
          20       0.74      0.80      0.77        92
          21       0.83      0.81      0.82       110
          22       0.80      0.84      0.82       305
          23       0.84      0.81      0.83       269
          24       0.85      0.89      0.87       269
          25       0.81      0.83      0.82       253
          26       0.82      0.83      0.83       185
          27       0.82      0.85      0.83       246
          28       0.84      0.88      0.86       221
          29       0.86      0.87      0.86       252
          30       0.88      0.89      0.89       215
          31       0.89      0.92      0.90       217
          32       0.88      0.85      0.86       204
          33       0.93      0.86      0.89       167
          34       0.83      0.84      0.83       140
          35       0.87      0.87      0.87       244
          36       0.94      0.94      0.94       156
          37       0.85      0.90      0.87       232
          38       0.85      0.85      0.85       301
          39       0.88      0.82      0.85       205
          40       0.77      0.78      0.78       328
          41       0.94      0.91      0.92       354
          42       0.80      0.80      0.80       185
          43       0.72      0.74      0.73       105
          44       0.72      0.75      0.74       114
          45       0.78      0.78      0.78       156
          46       0.81      0.81      0.81       130
          47       0.75      0.72      0.73       135
          48       0.80      0.78      0.79       109
          49       0.68      0.78      0.73        99
          50       0.81      0.83      0.82       110
          51       0.91      0.92      0.92       226
          52       0.86      0.73      0.79       142
          53       0.95      0.91      0.93       249
          54       0.79      0.84      0.81        93
          55       0.76      0.85      0.80       122
          56       0.79      0.79      0.79       116
          57       0.87      0.82      0.85        91
          58       0.80      0.71      0.75       106
          59       0.84      0.90      0.87       111
          60       0.93      0.90      0.91       126
          61       0.86      0.93      0.89       132
          62       0.83      0.78      0.80       121
          63       0.88      0.91      0.90       164
          64       0.95      0.95      0.95       101
          65       0.88      0.82      0.84       190

    accuracy                           0.83     11523
   macro avg       0.83      0.82      0.82     11523
weighted avg       0.83      0.83      0.83     11523

Logistic classification result:
               precision    recall  f1-score   support

           0       0.45      0.33      0.38       115
           1       0.53      0.42      0.47       141
           2       0.59      0.64      0.61       170
           3       0.63      0.59      0.61       227
           4       0.58      0.48      0.53       173
           5       0.51      0.54      0.53       215
           6       0.62      0.61      0.62       183
           7       0.45      0.38      0.42       193
           8       0.65      0.64      0.65       216
           9       0.42      0.32      0.36       152
          10       0.28      0.17      0.21        92
          11       0.30      0.25      0.27       186
          12       0.43      0.60      0.50       152
          13       0.50      0.54      0.52       151
          14       0.48      0.42      0.45       152
          15       0.65      0.62      0.63       138
          16       0.54      0.43      0.48       155
          17       0.55      0.78      0.65       130
          18       0.40      0.47      0.43       188
          19       0.31      0.24      0.27       196
          20       0.40      0.36      0.38        92
          21       0.48      0.40      0.44       110
          22       0.44      0.52      0.48       305
          23       0.52      0.54      0.53       269
          24       0.56      0.69      0.61       269
          25       0.48      0.55      0.51       253
          26       0.51      0.52      0.51       185
          27       0.58      0.57      0.57       246
          28       0.52      0.63      0.57       221
          29       0.51      0.59      0.55       252
          30       0.56      0.67      0.61       215
          31       0.62      0.67      0.65       217
          32       0.57      0.64      0.60       204
          33       0.57      0.49      0.52       167
          34       0.53      0.47      0.50       140
          35       0.63      0.53      0.57       244
          36       0.64      0.82      0.72       156
          37       0.64      0.64      0.64       232
          38       0.49      0.59      0.54       301
          39       0.65      0.72      0.69       205
          40       0.48      0.45      0.46       328
          41       0.70      0.79      0.74       354
          42       0.53      0.48      0.50       185
          43       0.49      0.41      0.45       105
          44       0.31      0.30      0.30       114
          45       0.42      0.62      0.50       156
          46       0.55      0.43      0.48       130
          47       0.47      0.24      0.32       135
          48       0.57      0.45      0.50       109
          49       0.46      0.27      0.34        99
          50       0.49      0.61      0.54       110
          51       0.67      0.88      0.76       226
          52       0.55      0.32      0.41       142
          53       0.72      0.79      0.75       249
          54       0.48      0.33      0.39        93
          55       0.49      0.33      0.39       122
          56       0.47      0.44      0.46       116
          57       0.52      0.66      0.58        91
          58       0.62      0.30      0.41       106
          59       0.61      0.42      0.50       111
          60       0.57      0.56      0.56       126
          61       0.56      0.68      0.61       132
          62       0.52      0.46      0.49       121
          63       0.72      0.66      0.69       164
          64       0.65      0.76      0.70       101
          65       0.64      0.51      0.56       190

    accuracy                           0.54     11523
   macro avg       0.53      0.52      0.52     11523
weighted avg       0.54      0.54      0.54     11523
